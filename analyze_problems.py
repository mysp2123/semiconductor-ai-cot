import json
import pandas as pd
from collections import Counter
import nltk
nltk.download('punkt_tab')
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

STOPWORDS = set(stopwords.words('english'))

# Äá»c dá»¯ liá»‡u tá»« file JSON Ä‘áº§u vÃ o
def load_data(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)

# Kiá»ƒm tra dá»¯ liá»‡u cÃ³ bá»‹ trÃ¹ng láº·p khÃ´ng
def check_duplicates(data):
    questions = [item["question"] for item in data]
    duplicate_counts = Counter(questions)
    duplicates = {q: c for q, c in duplicate_counts.items() if c > 1}
    return duplicates

# Kiá»ƒm tra cÃ¢u há»i/lá»i giáº£i bá»‹ thiáº¿u hoáº·c quÃ¡ ngáº¯n
def check_invalid_entries(data):
    invalid_entries = [item for item in data if len(item["question"]) < 10 or len(item["answer"]) < 5]
    return invalid_entries


# PhÃ¢n tÃ­ch sá»‘ lÆ°á»£ng cÃ¢u há»i vÃ  chá»§ Ä‘á» phá»• biáº¿n
def extract_keywords(text):
    words = nltk.word_tokenize(text.lower())
    words = [word for word in words if word.isalnum() and word not in STOPWORDS]
    return words

def analyze_data(data):
    print(f"Tá»•ng sá»‘ cÃ¢u há»i: {len(data)}")
    
    # Láº¥y cÃ¡c tá»« khÃ³a chÃ­nh trong cÃ¢u há»i
    all_keywords = []
    for item in data:
        all_keywords.extend(extract_keywords(item["question"]))
    
    common_topics = Counter(all_keywords).most_common(10)
    
    print("Chá»§ Ä‘á» phá»• biáº¿n:")
    for topic, count in common_topics:
        print(f"- {topic}: {count} láº§n")

# Lá»c dá»¯ liá»‡u Ä‘á»ƒ loáº¡i bá» cÃ¡c cÃ¢u khÃ´ng há»£p lá»‡
def clean_data(data):
    clean_data = [item for item in data if len(item["question"]) >= 10 and len(item["answer"]) >= 5]
    return clean_data

# LÆ°u dá»¯ liá»‡u sáº¡ch vÃ o file JSON
def save_clean_data(data, output_file):
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"âœ… ÄÃ£ lÆ°u dá»¯ liá»‡u sáº¡ch vÃ o {output_file}")

if __name__ == "__main__":
    input_file = "db/questions/cot_questions.json"
    output_file = "db/questions/cot_questions_clean.json"
    
    print("ğŸ” Äang phÃ¢n tÃ­ch dá»¯ liá»‡u...")
    data = load_data(input_file)
    
    analyze_data(data)
    
    duplicates = check_duplicates(data)
    if duplicates:
        print(f"âš ï¸ CÃ³ {len(duplicates)} cÃ¢u há»i bá»‹ trÃ¹ng láº·p")
    
    invalid_entries = check_invalid_entries(data)
    if invalid_entries:
        print(f"âš ï¸ CÃ³ {len(invalid_entries)} cÃ¢u há»i hoáº·c cÃ¢u tráº£ lá»i khÃ´ng há»£p lá»‡")
    
    print("ğŸ›  Äang lÃ m sáº¡ch dá»¯ liá»‡u...")
    cleaned_data = clean_data(data)
    
    save_clean_data(cleaned_data, output_file)
    print("âœ… HoÃ n thÃ nh!")
